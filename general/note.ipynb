{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform scrolling to load more job listings\n",
    "last_height = page.evaluate(\"document.body.scrollHeight\")\n",
    "while True:\n",
    "    # Scroll to the bottom of the page\n",
    "    page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait for some time to allow content to load\n",
    "    page.wait_for_timeout(1000)  # Adjust the timeout as needed\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = page.evaluate(\"document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        # If heights are the same, it means the page has reached the end\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "import json\n",
    "\n",
    "def main():\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=False)  # Set to False for debugging\n",
    "        page = browser.new_page()\n",
    "        base_url = \"https://weworkremotely.com\"\n",
    "\n",
    "        try:\n",
    "            # Navigate to the page with increased timeout\n",
    "            page.goto(base_url, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "\n",
    "            # Wait for job listings to load\n",
    "            page.wait_for_selector(\"section.jobs article li.new-listing-container\", timeout=60000)\n",
    "\n",
    "            # Select all job listing elements\n",
    "            job_elements = page.query_selector_all(\"section.jobs article li.new-listing-container\")\n",
    "            jobs = []\n",
    "\n",
    "            for job_element in job_elements:\n",
    "                try:\n",
    "                    # Extract title\n",
    "                    title_element = job_element.query_selector(\".new-listing__header__title\")\n",
    "                    title = title_element.inner_text().strip() if title_element else \"Unknown\"\n",
    "\n",
    "                    # Extract company link\n",
    "                    company_link_element = job_element.query_selector(\"div.tooltip--flag-logo a\")\n",
    "                    company_path = company_link_element.get_attribute(\"href\") if company_link_element else \"Unknown\"\n",
    "                    company_link = f\"{base_url}{company_path}\" if company_path.startswith(\"/\") else company_path\n",
    "\n",
    "                    # Extract apply link\n",
    "                    apply_link_element = job_element.query_selector(\"a[href^='/listings']\")\n",
    "                    apply_path = apply_link_element.get_attribute(\"href\") if apply_link_element else \"Unknown\"\n",
    "                    apply_link = f\"{base_url}{apply_path}\" if apply_path.startswith(\"/\") else apply_path\n",
    "\n",
    "                    # Extract company name\n",
    "                    company_element = job_element.query_selector(\".new-listing__company-name\")\n",
    "                    company = company_element.inner_text().strip() if company_element else \"Unknown\"\n",
    "\n",
    "                    # Extract location\n",
    "                    location_element = job_element.query_selector(\".new-listing__company-headquarters\")\n",
    "                    location = location_element.inner_text().strip() if location_element else \"Unknown\"\n",
    "\n",
    "                    # Extract job type\n",
    "                    job_type_element = job_element.query_selector(\".new-listing__categories__category\")\n",
    "                    job_type = job_type_element.inner_text().strip() if job_type_element else \"Unknown\"\n",
    "\n",
    "                    # Open the apply link in a new page\n",
    "                    new_page = browser.new_page()\n",
    "                    new_page.goto(apply_link, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "\n",
    "                    # Scrape details from the new page\n",
    "                    description_element = new_page.query_selector(\".lis-container__job__content\")\n",
    "                    description = description_element.inner_text().strip() if description_element else \"No description available\"\n",
    "\n",
    "                    # Close the new page\n",
    "                    new_page.close()\n",
    "\n",
    "                    # Append job data including details from the new page\n",
    "                    jobs.append({\n",
    "                        \"title\": title,\n",
    "                        \"companyLink\": company_link,\n",
    "                        \"applyLink\": apply_link,\n",
    "                        \"company\": company,\n",
    "                        \"location\": location,\n",
    "                        \"jobType\": job_type,\n",
    "                        \"description\": description\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred while processing a job element: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            jobs = []\n",
    "\n",
    "        finally:\n",
    "            browser.close()\n",
    "\n",
    "        # Output results in JSON format\n",
    "        print(json.dumps(jobs, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7695590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "async def scrape_jobs():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)  # Run in headless mode for speed\n",
    "        page = await browser.new_page()\n",
    "        base_url = \"https://weworkremotely.com\"\n",
    "\n",
    "        try:\n",
    "            # Navigate to the page with increased timeout\n",
    "            await page.goto(base_url, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "\n",
    "            # Wait for job listings to load\n",
    "            await page.wait_for_selector(\"section.jobs article li.new-listing-container\", timeout=60000)\n",
    "\n",
    "            # Select all job listing elements\n",
    "            job_elements = await page.query_selector_all(\"section.jobs article li.new-listing-container\")\n",
    "            jobs = []\n",
    "\n",
    "            async def process_job_element(job_element):\n",
    "                try:\n",
    "                    # Extract title\n",
    "                    title_element = await job_element.query_selector(\".new-listing__header__title\")\n",
    "                    title = await title_element.inner_text() if title_element else \"Unknown\"\n",
    "                    title = title.strip()\n",
    "\n",
    "                    # Extract company link\n",
    "                    company_link_element = await job_element.query_selector(\"div.tooltip--flag-logo a\")\n",
    "                    company_path = await company_link_element.get_attribute(\"href\") if company_link_element else \"Unknown\"\n",
    "                    company_link = f\"{base_url}{company_path}\" if company_path.startswith(\"/\") else company_path\n",
    "\n",
    "                    # Extract apply link\n",
    "                    apply_link_element = await job_element.query_selector(\"a[href^='/listings']\")\n",
    "                    apply_path = await apply_link_element.get_attribute(\"href\") if apply_link_element else \"Unknown\"\n",
    "                    apply_link = f\"{base_url}{apply_path}\" if apply_path.startswith(\"/\") else apply_link\n",
    "\n",
    "                    # Extract company name\n",
    "                    company_element = await job_element.query_selector(\".new-listing__company-name\")\n",
    "                    company = await company_element.inner_text() if company_element else \"Unknown\"\n",
    "                    company = company.strip()\n",
    "\n",
    "                    # Extract location\n",
    "                    location_element = await job_element.query_selector(\".new-listing__company-headquarters\")\n",
    "                    location = await location_element.inner_text() if location_element else \"Unknown\"\n",
    "                    location = location.strip()\n",
    "\n",
    "                    # Extract job type\n",
    "                    job_type_element = await job_element.query_selector(\".new-listing__categories__category\")\n",
    "                    job_type = await job_type_element.inner_text() if job_type_element else \"Unknown\"\n",
    "                    job_type = job_type.strip()\n",
    "\n",
    "                    # Open the apply link in a new page\n",
    "                    new_page = await browser.new_page()\n",
    "                    await new_page.goto(apply_link, timeout=30000, wait_until=\"domcontentloaded\")\n",
    "\n",
    "                    # Scrape details from the new page\n",
    "                    description_element = await new_page.query_selector(\".listing-container .listing-header\")\n",
    "                    description = await description_element.inner_text() if description_element else \"No description available\"\n",
    "                    description = description.strip()\n",
    "\n",
    "                    # Close the new page\n",
    "                    await new_page.close()\n",
    "\n",
    "                    return {\n",
    "                        \"title\": title,\n",
    "                        \"companyLink\": company_link,\n",
    "                        \"applyLink\": apply_link,\n",
    "                        \"company\": company,\n",
    "                        \"location\": location,\n",
    "                        \"jobType\": job_type,\n",
    "                        \"description\": description\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred while processing a job element: {str(e)}\")\n",
    "                    return None\n",
    "\n",
    "            # Process job elements concurrently\n",
    "            tasks = [process_job_element(job_element) for job_element in job_elements]\n",
    "            jobs = await asyncio.gather(*tasks)\n",
    "            jobs = [job for job in jobs if job is not None]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            jobs = []\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "        # Output results in JSON format\n",
    "        print(json.dumps(jobs, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(scrape_jobs())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scroll\n",
    "\n",
    "\n",
    "from playwright.sync_api import sync_playwright\n",
    "import json\n",
    "\n",
    "def main():\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=False)  # Set to False for debugging\n",
    "        page = browser.new_page()\n",
    "        base_url = \"https://weworkremotely.com\"\n",
    "\n",
    "        try:\n",
    "            # Navigate to the page with increased timeout\n",
    "            page.goto(base_url, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "\n",
    "            # Perform scrolling to load more job listings\n",
    "            last_height = page.evaluate(\"document.body.scrollHeight\")\n",
    "            while True:\n",
    "                # Scroll to the bottom of the page\n",
    "                page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                # Wait for some time to allow content to load\n",
    "                page.wait_for_timeout(2000)  # Adjust the timeout as needed\n",
    "\n",
    "                # Calculate new scroll height and compare with last scroll height\n",
    "                new_height = page.evaluate(\"document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    # If heights are the same, it means the page has reached the end\n",
    "                    break\n",
    "                last_height = new_height\n",
    "\n",
    "            # Wait for job listings to load\n",
    "            page.wait_for_selector(\"section.jobs article li.new-listing-container\", timeout=60000)\n",
    "\n",
    "            # Select all job listing elements\n",
    "            job_elements = page.query_selector_all(\"section.jobs article li.new-listing-container\")\n",
    "            jobs = []\n",
    "\n",
    "            for job_element in job_elements:\n",
    "                try:\n",
    "                    # Extract title\n",
    "                    title_element = job_element.query_selector(\".new-listing__header__title\")\n",
    "                    title = title_element.inner_text().strip() if title_element else \"Unknown\"\n",
    "\n",
    "                    # Extract company link\n",
    "                    company_link_element = job_element.query_selector(\"div.tooltip--flag-logo a\")\n",
    "                    company_path = company_link_element.get_attribute(\"href\") if company_link_element else \"Unknown\"\n",
    "                    company_link = f\"{base_url}{company_path}\" if company_path.startswith(\"/\") else company_path\n",
    "\n",
    "                    # Extract apply link\n",
    "                    apply_link_element = job_element.query_selector(\"a[href^='/listings']\")\n",
    "                    apply_path = apply_link_element.get_attribute(\"href\") if apply_link_element else \"Unknown\"\n",
    "                    apply_link = f\"{base_url}{apply_path}\" if apply_path.startswith(\"/\") else apply_path\n",
    "\n",
    "                    # Extract company name\n",
    "                    company_element = job_element.query_selector(\".new-listing__company-name\")\n",
    "                    company = company_element.inner_text().strip() if company_element else \"Unknown\"\n",
    "\n",
    "                    # Extract location\n",
    "                    location_element = job_element.query_selector(\".new-listing__company-headquarters\")\n",
    "                    location = location_element.inner_text().strip() if location_element else \"Unknown\"\n",
    "\n",
    "                    # Extract job type\n",
    "                    job_type_element = job_element.query_selector(\".new-listing__categories__category\")\n",
    "                    job_type = job_type_element.inner_text().strip() if job_type_element else \"Unknown\"\n",
    "\n",
    "                    # Create a new browser context for the apply link\n",
    "                    with browser.new_context() as new_context:\n",
    "                        new_page = new_context.new_page()\n",
    "                        new_page.goto(apply_link, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "\n",
    "                        # Scrape details from the new page\n",
    "                        description_element = new_page.query_selector(\".lis-container__job__content\")\n",
    "                        description = description_element.inner_text().strip() if description_element else \"No description available\"\n",
    "\n",
    "                        # Close the new page\n",
    "                        new_page.close()\n",
    "\n",
    "                    # Append job data including details from the new page\n",
    "                    jobs.append({\n",
    "                        \"title\": title,\n",
    "                        \"companyLink\": company_link,\n",
    "                        \"applyLink\": apply_link,\n",
    "                        \"company\": company,\n",
    "                        \"location\": location,\n",
    "                        \"jobType\": job_type,\n",
    "                        \"description\": description\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred while processing a job element: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            jobs = []\n",
    "\n",
    "        finally:\n",
    "            browser.close()\n",
    "\n",
    "        # Output results in JSON format\n",
    "        print(json.dumps(jobs, indent=2))\n",
    "        print(f\"Total jobs found: {len(jobs)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "import json\n",
    "\n",
    "def main():\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=False)  # Set to False for debugging\n",
    "        page = browser.new_page()\n",
    "        base_url = \"https://weworkremotely.com\"\n",
    "\n",
    "        try:\n",
    "            # Navigate to the page with increased timeout\n",
    "            page.goto(base_url, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "\n",
    "            # Wait for job listings to load\n",
    "            page.wait_for_selector(\"section.jobs article li.new-listing-container\", timeout=60000)\n",
    "\n",
    "            # Select all job listing elements\n",
    "            job_elements = page.query_selector_all(\"section.jobs article li.new-listing-container\")\n",
    "            jobs = []\n",
    "\n",
    "            for job_element in job_elements:\n",
    "                try:\n",
    "                    # Extract title\n",
    "                    title_element = job_element.query_selector(\".new-listing__header__title\")\n",
    "                    title = title_element.inner_text().strip() if title_element else \"Unknown\"\n",
    "\n",
    "                    # Extract company link\n",
    "                    company_link_element = job_element.query_selector(\"div.tooltip--flag-logo a\")\n",
    "                    company_path = company_link_element.get_attribute(\"href\") if company_link_element else \"Unknown\"\n",
    "                    company_link = f\"{base_url}{company_path}\" if company_path.startswith(\"/\") else company_path\n",
    "\n",
    "                    # Extract apply link\n",
    "                    apply_link_element = job_element.query_selector(\"a[href^='/listings']\")\n",
    "                    apply_path = apply_link_element.get_attribute(\"href\") if apply_link_element else \"Unknown\"\n",
    "                    apply_link = f\"{base_url}{apply_path}\" if apply_path.startswith(\"/\") else apply_path\n",
    "\n",
    "                    # Extract company name\n",
    "                    company_element = job_element.query_selector(\".new-listing__company-name\")\n",
    "                    company = company_element.inner_text().strip() if company_element else \"Unknown\"\n",
    "\n",
    "                    # Extract location\n",
    "                    location_element = job_element.query_selector(\".new-listing__company-headquarters\")\n",
    "                    location = location_element.inner_text().strip() if location_element else \"Unknown\"\n",
    "\n",
    "                    # Extract job type\n",
    "                    job_type_element = job_element.query_selector(\".new-listing__categories__category\")\n",
    "                    job_type = job_type_element.inner_text().strip() if job_type_element else \"Unknown\"\n",
    "\n",
    "                    # Create a new browser context for the apply link\n",
    "                    with browser.contexts[0].browser.new_context() as new_context:\n",
    "                        new_page = new_context.new_page()\n",
    "                        new_page.goto(apply_link, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "\n",
    "                        # Scrape details from the new page\n",
    "                        description_element = new_page.query_selector(\".lis-container__job__content\")\n",
    "                        description = description_element.inner_text().strip() if description_element else \"No description available\"\n",
    "\n",
    "                        # Close the new page\n",
    "                        new_page.close()\n",
    "\n",
    "                    # Append job data including details from the new page\n",
    "                    jobs.append({\n",
    "                        \"title\": title,\n",
    "                        \"companyLink\": company_link,\n",
    "                        \"applyLink\": apply_link,\n",
    "                        \"company\": company,\n",
    "                        \"location\": location,\n",
    "                        \"jobType\": job_type,\n",
    "                        \"description\": description\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred while processing a job element: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            jobs = []\n",
    "\n",
    "        finally:\n",
    "            browser.close()\n",
    "\n",
    "        # Output results in JSON format\n",
    "        print(json.dumps(jobs, indent=2))\n",
    "        with open(\"jobs scraped from weworkremotely.json\", \"w\") as f:\n",
    "            json.dump(jobs, f, indent=2)\n",
    "        print(f\"Total jobs found: {len(jobs)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294caea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grok\n",
    "\n",
    "\n",
    "import json\n",
    "import time\n",
    "from playwright.sync_api import sync_playwright, TimeoutError\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def retry_operation(operation, max_attempts=3, delay=2):\n",
    "    \"\"\"Retry a Playwright operation with exponential backoff.\"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            return operation()\n",
    "        except TimeoutError as e:\n",
    "            if attempt == max_attempts - 1:\n",
    "                raise e\n",
    "            time.sleep(delay * (2 ** attempt))\n",
    "            logger.warning(f\"Retrying operation (attempt {attempt + 1}/{max_attempts})\")\n",
    "\n",
    "def scrape_job_details(page, job_data, base_url):\n",
    "    \"\"\"Scrape job details from the apply link in a new tab.\"\"\"\n",
    "    try:\n",
    "        # Open new tab\n",
    "        new_page = page.context.new_page()\n",
    "        \n",
    "        # Navigate to apply link with retry\n",
    "        retry_operation(lambda: new_page.goto(\n",
    "            job_data['applyLink'], \n",
    "            timeout=30000, \n",
    "            wait_until=\"domcontentloaded\"\n",
    "        ))\n",
    "        \n",
    "        # Scrape description\n",
    "        description = retry_operation(lambda: new_page.query_selector(\n",
    "            \".lis-container__job__content\"\n",
    "        ))\n",
    "        job_data['description'] = description.inner_text().strip() if description else \"No description available\"\n",
    "        \n",
    "        return job_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping details for {job_data['title']}: {str(e)}\")\n",
    "        job_data['description'] = \"Error retrieving description\"\n",
    "        return job_data\n",
    "    finally:\n",
    "        new_page.close()\n",
    "\n",
    "def main():\n",
    "    jobs = []\n",
    "    base_url = \"https://weworkremotely.com\"\n",
    "    \n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)  # Headless for production\n",
    "        context = browser.new_context(\n",
    "            viewport={'width': 1280, 'height': 720},\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "        )\n",
    "        page = context.new_page()\n",
    "\n",
    "        try:\n",
    "            # Navigate to main page with retry\n",
    "            logger.info(\"Navigating to main page\")\n",
    "            retry_operation(lambda: page.goto(\n",
    "                base_url, \n",
    "                timeout=30000, \n",
    "                wait_until=\"domcontentloaded\"\n",
    "            ))\n",
    "\n",
    "            # Wait for job listings\n",
    "            retry_operation(lambda: page.wait_for_selector(\n",
    "                \"section.jobs article li.new-listing-container\", \n",
    "                timeout=30000\n",
    "            ))\n",
    "\n",
    "            # Extract job listings\n",
    "            job_elements = page.query_selector_all(\"section.jobs article li.new-listing-container\")\n",
    "            logger.info(f\"Found {len(job_elements)} job listings\")\n",
    "\n",
    "            # Prepare job data\n",
    "            for job_element in job_elements:\n",
    "                try:\n",
    "                    title = job_element.query_selector(\".new-listing__header__title\")\n",
    "                    company_link = job_element.query_selector(\"div.tooltip--flag-logo a\")\n",
    "                    apply_link = job_element.query_selector(\"a[href^='/listings']\")\n",
    "                    company = job_element.query_selector(\".new-listing__company-name\")\n",
    "                    location = job_element.query_selector(\".new-listing__company-headquarters\")\n",
    "                    job_type = job_element.query_selector(\".new-listing__categories__category\")\n",
    "\n",
    "                    job_data = {\n",
    "                        \"title\": title.inner_text().strip() if title else \"Unknown\",\n",
    "                        \"companyLink\": f\"{base_url}{company_link.get_attribute('href')}\" \n",
    "                            if company_link and company_link.get_attribute('href').startswith(\"/\") \n",
    "                            else company_link.get_attribute('href') if company_link else \"Unknown\",\n",
    "                        \"applyLink\": f\"{base_url}{apply_link.get_attribute('href')}\" \n",
    "                            if apply_link and apply_link.get_attribute('href').startswith(\"/\") \n",
    "                            else apply_link.get_attribute('href') if apply_link else \"Unknown\",\n",
    "                        \"company\": company.inner_text().strip() if company else \"Unknown\",\n",
    "                        \"location\": location.inner_text().strip() if location else \"Unknown\",\n",
    "                        \"jobType\": job_type.inner_text().strip() if job_type else \"Unknown\",\n",
    "                        \"description\": \"\"  # Will be filled later\n",
    "                    }\n",
    "                    jobs.append(job_data)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing job element: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            # Scrape job details in parallel\n",
    "            max_workers = min(len(jobs), 5)  # Limit concurrent tabs\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                future_to_job = {\n",
    "                    executor.submit(scrape_job_details, page, job_data, base_url): job_data \n",
    "                    for job_data in jobs\n",
    "                }\n",
    "                \n",
    "                for future in as_completed(future_to_job):\n",
    "                    try:\n",
    "                        job_data = future.result()\n",
    "                        time.sleep(0.5)  # Rate limiting\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error in parallel processing: {str(e)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fatal error occurred: {str(e)}\")\n",
    "            jobs = []\n",
    "        \n",
    "        finally:\n",
    "            context.close()\n",
    "            browser.close()\n",
    "\n",
    "    # Save results\n",
    "    output_file = \"jobs_scraped_from_weworkremotely.json\"\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(jobs, f, indent=2, ensure_ascii=False)\n",
    "    logger.info(f\"Saved {len(jobs)} jobs to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
